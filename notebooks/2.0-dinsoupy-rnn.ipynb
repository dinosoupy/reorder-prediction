{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "284c2fa6-f764-4ad5-be98-003760ae6161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/anish/repos/reorder-prediction\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: reorder-prediction\n",
      "  Running setup.py develop for reorder-prediction\n",
      "Successfully installed reorder-prediction-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -e ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5210cd95-37a5-49fe-8ece-9bf8868e5373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import data_frame as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee88aa-e0e2-4452-89fb-4001d6e993a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'user_id',\n",
    "            'product_id',\n",
    "            'aisle_id',\n",
    "            'department_id',\n",
    "            'is_ordered_history',\n",
    "            'index_in_order_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'order_size_history',\n",
    "            'reorder_size_history',\n",
    "            'order_number_history',\n",
    "            'history_length',\n",
    "            'product_name',\n",
    "            'product_name_length',\n",
    "            'eval_set',\n",
    "            'label'\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r' for i in data_cols]\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "                        \n",
    "        print(self.test_df.shape)\n",
    "        print('loaded data')\n",
    "                        \n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.8)\n",
    "                \n",
    "        print 'train size', len(self.train_df)\n",
    "        print 'val size', len(self.val_df)\n",
    "        print 'test size', len(self.test_df)\n",
    "                        \n",
    "        def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "            batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "            for batch in batch_gen:\n",
    "                batch['order_dow_history'] = np.roll(batch['order_dow_history'], -1, axis=1)\n",
    "                batch['order_hour_history'] = np.roll(batch['order_hour_history'], -1, axis=1)\n",
    "                batch['days_since_prior_order_history'] = np.roll(batch['days_since_prior_order_history'], -1, axis=1)\n",
    "                batch['order_number_history'] = np.roll(batch['order_number_history'], -1, axis=1)\n",
    "                batch['next_is_ordered'] = np.roll(batch['is_ordered_history'], -1, axis=1)\n",
    "                batch['is_none'] = batch['product_id'] == 0\n",
    "                if not is_test:\n",
    "                    batch['history_length'] = batch['history_length'] - 1\n",
    "                yield batch\n",
    "        \n",
    "        def train_batch_generator(self, batch_size):\n",
    "            return self.batch_generator(\n",
    "                batch_size=batch_size,\n",
    "                df=self.train_df,\n",
    "                shuffle=True,\n",
    "                num_epochs=10000,\n",
    "                is_test=False\n",
    "            )\n",
    "\n",
    "        def val_batch_generator(self, batch_size):\n",
    "            return self.batch_generator(\n",
    "                batch_size=batch_size,\n",
    "                df=self.val_df,\n",
    "                shuffle=True,\n",
    "                num_epochs=10000,\n",
    "                is_test=False\n",
    "            )\n",
    "\n",
    "        def test_batch_generator(self, batch_size):\n",
    "            return self.batch_generator(\n",
    "                batch_size=batch_size,\n",
    "                df=self.test_df,\n",
    "                shuffle=False,\n",
    "                num_epochs=1,\n",
    "                is_test=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4151645-e9b0-47f9-b304-7fd5e0b26fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(TFBaseModel):\n",
    "\n",
    "    def __init__(self, lstm_size, dilations, filter_widths, skip_channels, residual_channels, **kwargs):\n",
    "        self.lstm_size = lstm_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_widths = filter_widths\n",
    "        self.skip_channels = skip_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        super(rnn, self).__init__(**kwargs)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        x = self.get_input_sequences()\n",
    "        preds = self.calculate_outputs(x)\n",
    "        loss = sequence_log_loss(self.next_is_ordered, preds, self.history_length, 100)\n",
    "        return loss\n",
    "\n",
    "    def get_input_sequences(self):\n",
    "        self.user_id = tf.placeholder(tf.int32, [None])\n",
    "        self.product_id = tf.placeholder(tf.int32, [None])\n",
    "        self.aisle_id = tf.placeholder(tf.int32, [None])\n",
    "        self.department_id = tf.placeholder(tf.int32, [None])\n",
    "        self.is_none = tf.placeholder(tf.int32, [None])\n",
    "        self.history_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        self.is_ordered_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.index_in_order_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_dow_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_hour_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.days_since_prior_order_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_size_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.reorder_size_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_number_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.product_name = tf.placeholder(tf.int32, [None, 30])\n",
    "        self.product_name_length = tf.placeholder(tf.int32, [None])\n",
    "        self.next_is_ordered = tf.placeholder(tf.int32, [None, 100])\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # product data\n",
    "        product_embeddings = tf.get_variable(\n",
    "            name='product_embeddings',\n",
    "            shape=[50000, self.lstm_size],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        aisle_embeddings = tf.get_variable(\n",
    "            name='aisle_embeddings',\n",
    "            shape=[250, 50],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        department_embeddings = tf.get_variable(\n",
    "            name='department_embeddings',\n",
    "            shape=[50, 10],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        product_names = tf.one_hot(self.product_name, 2532)\n",
    "        product_names = tf.reduce_max(product_names, 1)\n",
    "        product_names = dense_layer(product_names, 100, activation=tf.nn.relu)\n",
    "\n",
    "        is_none = tf.cast(tf.expand_dims(self.is_none, 1), tf.float32)\n",
    "\n",
    "        x_product = tf.concat([\n",
    "            tf.nn.embedding_lookup(product_embeddings, self.product_id),\n",
    "            tf.nn.embedding_lookup(aisle_embeddings, self.aisle_id),\n",
    "            tf.nn.embedding_lookup(department_embeddings, self.department_id),\n",
    "            is_none,\n",
    "            product_names\n",
    "        ], axis=1)\n",
    "        x_product = tf.tile(tf.expand_dims(x_product, 1), (1, 100, 1))\n",
    "\n",
    "        # user data\n",
    "        user_embeddings = tf.get_variable(\n",
    "            name='user_embeddings',\n",
    "            shape=[207000, self.lstm_size],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        x_user = tf.nn.embedding_lookup(user_embeddings, self.user_id)\n",
    "        x_user = tf.tile(tf.expand_dims(x_user, 1), (1, 100, 1))\n",
    "\n",
    "        # sequence data\n",
    "        is_ordered_history = tf.one_hot(self.is_ordered_history, 2)\n",
    "        index_in_order_history = tf.one_hot(self.index_in_order_history, 20)\n",
    "        order_dow_history = tf.one_hot(self.order_dow_history, 8)\n",
    "        order_hour_history = tf.one_hot(self.order_hour_history, 25)\n",
    "        days_since_prior_order_history = tf.one_hot(self.days_since_prior_order_history, 31)\n",
    "        order_size_history = tf.one_hot(self.order_size_history, 60)\n",
    "        reorder_size_history = tf.one_hot(self.reorder_size_history, 50)\n",
    "        order_number_history = tf.one_hot(self.order_number_history, 101)\n",
    "\n",
    "        index_in_order_history_scalar = tf.expand_dims(tf.cast(self.index_in_order_history, tf.float32) / 20.0, 2)\n",
    "        order_dow_history_scalar = tf.expand_dims(tf.cast(self.order_dow_history, tf.float32) / 8.0, 2)\n",
    "        order_hour_history_scalar = tf.expand_dims(tf.cast(self.order_hour_history, tf.float32) / 25.0, 2)\n",
    "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(self.days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "        order_size_history_scalar = tf.expand_dims(tf.cast(self.order_size_history, tf.float32) / 60.0, 2)\n",
    "        reorder_size_history_scalar = tf.expand_dims(tf.cast(self.reorder_size_history, tf.float32) / 50.0, 2)\n",
    "        order_number_history_scalar = tf.expand_dims(tf.cast(self.order_number_history, tf.float32) / 100.0, 2)\n",
    "\n",
    "        x_history = tf.concat([\n",
    "            is_ordered_history,\n",
    "            index_in_order_history,\n",
    "            order_dow_history,\n",
    "            order_hour_history,\n",
    "            days_since_prior_order_history,\n",
    "            order_size_history,\n",
    "            reorder_size_history,\n",
    "            order_number_history,\n",
    "            index_in_order_history_scalar,\n",
    "            order_dow_history_scalar,\n",
    "            order_hour_history_scalar,\n",
    "            days_since_prior_order_history_scalar,\n",
    "            order_size_history_scalar,\n",
    "            reorder_size_history_scalar,\n",
    "            order_number_history_scalar,\n",
    "        ], axis=2)\n",
    "\n",
    "        x = tf.concat([x_history, x_product, x_user], axis=2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def calculate_outputs(self, x):\n",
    "        h = lstm_layer(x, self.history_length, self.lstm_size)\n",
    "        c = wavenet(x, self.dilations, self.filter_widths, self.skip_channels, self.residual_channels)\n",
    "        h = tf.concat([h, c, x], axis=2)\n",
    "\n",
    "        self.h_final = time_distributed_dense_layer(h, 50, activation=tf.nn.relu, scope='dense-1')\n",
    "        y_hat = time_distributed_dense_layer(self.h_final, 1, activation=tf.nn.sigmoid, scope='dense-2')\n",
    "        y_hat = tf.squeeze(y_hat, 2)\n",
    "\n",
    "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), self.history_length - 1], axis=1)\n",
    "        self.final_states = tf.gather_nd(self.h_final, final_temporal_idx)\n",
    "        self.final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "\n",
    "        self.prediction_tensors = {\n",
    "            'user_ids': self.user_id,\n",
    "            'product_ids': self.product_id,\n",
    "            'final_states': self.final_states,\n",
    "            'predictions': self.final_predictions\n",
    "        }\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd304366-ab13-40e3-80e9-9ee951a3e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../raw/data'\n",
    "\n",
    "dr = DataReader(data_dir=data_dir)\n",
    "\n",
    "nn = rnn(\n",
    "    reader=dr,\n",
    "    log_dir=os.path.join(base_dir, 'logs'),\n",
    "    checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n",
    "    prediction_dir=os.path.join(base_dir, 'predictions'),\n",
    "    optimizer='adam',\n",
    "    learning_rate=.001,\n",
    "    lstm_size=300,\n",
    "    dilations=[2**i for i in range(6)],\n",
    "    filter_widths=[2]*6,\n",
    "    skip_channels=64,\n",
    "    residual_channels=128,\n",
    "    batch_size=128,\n",
    "    num_training_steps=200000,\n",
    "    early_stopping_steps=30000,\n",
    "    warm_start_init_step=0,\n",
    "    regularization_constant=0.0,\n",
    "    keep_prob=1.0,\n",
    "    enable_parameter_averaging=False,\n",
    "    num_restarts=2,\n",
    "    min_steps_to_checkpoint=5000,\n",
    "    log_interval=20,\n",
    "    num_validation_batches=4,\n",
    ")\n",
    "nn.fit()\n",
    "nn.restore()\n",
    "nn.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
