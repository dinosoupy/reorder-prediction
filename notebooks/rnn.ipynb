{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5210cd95-37a5-49fe-8ece-9bf8868e5373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from data_frame import DataFrame\n",
    "import tabnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d46a040d-73dc-4a63-994e-2adc4e138620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                               (13514162,)\n",
      "product_id                            (13514162,)\n",
      "aisle_id                              (13514162,)\n",
      "department_id                         (13514162,)\n",
      "is_ordered_history                (13514162, 100)\n",
      "index_in_order_history            (13514162, 100)\n",
      "order_dow_history                 (13514162, 100)\n",
      "order_hour_history                (13514162, 100)\n",
      "days_since_prior_order_history    (13514162, 100)\n",
      "order_size_history                (13514162, 100)\n",
      "reorder_size_history              (13514162, 100)\n",
      "order_number_history              (13514162, 100)\n",
      "history_length                        (13514162,)\n",
      "product_name                       (13514162, 30)\n",
      "product_name_length                   (13514162,)\n",
      "eval_set                              (13514162,)\n",
      "label                                 (13514162,)\n",
      "dtype: object\n",
      "loaded data\n",
      "train size 10811329\n",
      "val size 2702833\n",
      "test size 13514162\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data/interim'\n",
    "data_cols = [\n",
    "            'user_id',\n",
    "            'product_id',\n",
    "            'aisle_id',\n",
    "            'department_id',\n",
    "            'is_ordered_history',\n",
    "            'index_in_order_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'order_size_history',\n",
    "            'reorder_size_history',\n",
    "            'order_number_history',\n",
    "            'history_length',\n",
    "            'product_name',\n",
    "            'product_name_length',\n",
    "            'eval_set',\n",
    "            'label'\n",
    "        ]\n",
    "\n",
    "data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "test_df = DataFrame(columns=data_cols, data=data)\n",
    "                        \n",
    "print(test_df.shapes())\n",
    "print('loaded data')\n",
    "\n",
    "train_df, val_df = test_df.train_test_split(train_size=0.8)\n",
    "\n",
    "print('train size', len(train_df))\n",
    "print('val size', len(val_df))\n",
    "print('test size', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e58ec2-b7b9-4bce-b5b6-01190cac6c47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input layers\n",
    "user_id = Input(shape=(), dtype=tf.int32, name='user_id')\n",
    "product_id = Input(shape=(), dtype=tf.int32, name='product_id')\n",
    "aisle_id = Input(shape=(), dtype=tf.int32, name='aisle_id')\n",
    "department_id = Input(shape=(), dtype=tf.int32, name='department_id')\n",
    "is_none = Input(shape=(), dtype=tf.int32, name='is_none')\n",
    "history_length = Input(shape=(), dtype=tf.int32, name='history_length')\n",
    "\n",
    "is_ordered_history = Input(shape=(100,), dtype=tf.int32, name='is_ordered_history')\n",
    "index_in_order_history = Input(shape=(100,), dtype=tf.int32, name='index_in_order_history')\n",
    "order_dow_history = Input(shape=(100,), dtype=tf.int32, name='order_dow_history')\n",
    "order_hour_history = Input(shape=(100,), dtype=tf.int32, name='order_hour_history')\n",
    "days_since_prior_order_history = Input(shape=(100,), dtype=tf.int32, name='days_since_prior_order_history')\n",
    "order_size_history = Input(shape=(100,), dtype=tf.int32, name='order_size_history')\n",
    "reorder_size_history = Input(shape=(100,), dtype=tf.int32, name='reorder_size_history')\n",
    "order_number_history = Input(shape=(100,), dtype=tf.int32, name='order_number_history')\n",
    "product_name = Input(shape=(30,), dtype=tf.int32, name='product_name')\n",
    "product_name_length = Input(shape=(), dtype=tf.int32, name='product_name_length')\n",
    "next_is_ordered = Input(shape=(100,), dtype=tf.int32, name='next_is_ordered')\n",
    "\n",
    "# Product data\n",
    "product_embeddings = Embedding(input_dim=50000, output_dim=lstm_size, name='product_embeddings')(product_id)\n",
    "aisle_embeddings = Embedding(input_dim=250, output_dim=50, name='aisle_embeddings')(aisle_id)\n",
    "department_embeddings = Embedding(input_dim=50, output_dim=10, name='department_embeddings')(department_id)\n",
    "\n",
    "# One-hot encoding and reduction\n",
    "product_names = tf.one_hot(product_name, 2532)\n",
    "product_names = tf.reduce_max(product_names, 1)\n",
    "product_names = dense_layer(product_names, 100, activation=tf.nn.relu)\n",
    "\n",
    "# Cast and expand dimensions\n",
    "is_none = tf.cast(tf.expand_dims(is_none, 1), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2e61fe0-cd60-4d52-97f1-3f03dc187a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate all the inputs\n",
    "x_product = Concatenate(axis=1)([\n",
    "    product_embeddings,\n",
    "    aisle_embeddings,\n",
    "    department_embeddings,\n",
    "    is_none,\n",
    "    product_names\n",
    "])\n",
    "\n",
    "# Tile the tensor\n",
    "x_product = tf.tile(tf.expand_dims(x_product, 1), (1, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8cd86d2-f5c8-43f5-ada3-23a329d4a7ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User data\n",
    "user_embeddings = Embedding(input_dim=207000, output_dim=lstm_size, name='user_embeddings')(user_id)\n",
    "x_user = tf.expand_dims(user_embeddings, 1)\n",
    "x_user = tf.tile(x_user, (1, 100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e41953fb-faea-4261-9e93-fcae06147e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sequence data\n",
    "is_ordered_history = tf.one_hot(is_ordered_history, depth=2)\n",
    "index_in_order_history = tf.one_hot(index_in_order_history, depth=20)\n",
    "order_dow_history = tf.one_hot(order_dow_history, depth=8)\n",
    "order_hour_history = tf.one_hot(order_hour_history, depth=25)\n",
    "days_since_prior_order_history = tf.one_hot(days_since_prior_order_history, depth=31)\n",
    "order_size_history = tf.one_hot(order_size_history, depth=60)\n",
    "reorder_size_history = tf.one_hot(reorder_size_history, depth=50)\n",
    "order_number_history = tf.one_hot(order_number_history, depth=101)\n",
    "\n",
    "index_in_order_history_scalar = tf.cast(index_in_order_history, tf.float32) / 20.0\n",
    "order_dow_history_scalar = tf.cast(order_dow_history, tf.float32) / 8.0\n",
    "order_hour_history_scalar = tf.cast(order_hour_history, tf.float32) / 25.0\n",
    "days_since_prior_order_history_scalar = tf.cast(days_since_prior_order_history, tf.float32) / 31.0\n",
    "order_size_history_scalar = tf.cast(order_size_history, tf.float32) / 60.0\n",
    "reorder_size_history_scalar = tf.cast(reorder_size_history, tf.float32) / 50.0\n",
    "order_number_history_scalar = tf.cast(order_number_history, tf.float32) / 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f07abd5e-7dbd-4756-a292-9debfe6865dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenating all histories\n",
    "x_history = Concatenate(axis=2)([\n",
    "    is_ordered_history,\n",
    "    index_in_order_history,\n",
    "    order_dow_history,\n",
    "    order_hour_history,\n",
    "    days_since_prior_order_history,\n",
    "    order_size_history,\n",
    "    reorder_size_history,\n",
    "    order_number_history,\n",
    "    index_in_order_history_scalar,\n",
    "    order_dow_history_scalar,\n",
    "    order_hour_history_scalar,\n",
    "    days_since_prior_order_history_scalar,\n",
    "    order_size_history_scalar,\n",
    "    reorder_size_history_scalar,\n",
    "    order_number_history_scalar\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00c8c412-0778-48c3-bbc8-2ffd524c848f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate all inputs\n",
    "x = Concatenate(axis=2)([x_history, x_product, x_user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b664118e-4d2d-4f2e-92ca-e7e68533fc3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 100, 1353])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787f4771-11dc-4f48-a5b0-beee54667a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm = LSTM(\n",
    "    units=lstm_size,\n",
    "    return_sequences=True,\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9d4e980-f3d3-4320-8ca3-901632575129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "y_hat = lstm(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=x, outputs=y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2266889d-037e-4c62-a555-316972e9ce8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 1353)]       0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 300)          1984800   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1984800 (7.57 MB)\n",
      "Trainable params: 1984800 (7.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f48d583f-530d-4be7-a2f5-3c81a6cfa29d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                       tf.keras.metrics.FalseNegatives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c6337bb-b647-40bb-a320-c8a9b2602a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "    batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "    for batch in batch_gen:\n",
    "        batch['order_dow_history'] = np.roll(batch['order_dow_history'], -1, axis=1)\n",
    "        batch['order_hour_history'] = np.roll(batch['order_hour_history'], -1, axis=1)\n",
    "        batch['days_since_prior_order_history'] = np.roll(batch['days_since_prior_order_history'], -1, axis=1)\n",
    "        batch['order_number_history'] = np.roll(batch['order_number_history'], -1, axis=1)\n",
    "        batch['next_is_ordered'] = np.roll(batch['is_ordered_history'], -1, axis=1)\n",
    "        batch['is_none'] = batch['product_id'] == 0\n",
    "        if not is_test:\n",
    "            batch['history_length'] = batch['history_length'] - 1\n",
    "        yield batch\n",
    "        \n",
    "def train_batch_generator(batch_size):\n",
    "    return batch_generator(\n",
    "        batch_size=batch_size,\n",
    "        df=train_df,\n",
    "        shuffle=True,\n",
    "        num_epochs=10000,\n",
    "        is_test=False\n",
    "    )\n",
    "\n",
    "def val_batch_generator(batch_size):\n",
    "    return batch_generator(\n",
    "        batch_size=batch_size,\n",
    "        df=val_df,\n",
    "        shuffle=True,\n",
    "        num_epochs=10000,\n",
    "        is_test=False\n",
    "    )\n",
    "\n",
    "def test_batch_generator(batch_size):\n",
    "    return batch_generator(\n",
    "        batch_size=batch_size,\n",
    "        df=test_df,\n",
    "        shuffle=False,\n",
    "        num_epochs=1,\n",
    "        is_test=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "098754fe-44ad-49fb-95e7-6e25ff0a0334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_training_steps = 10\n",
    "batch_size = 128\n",
    "num_validation_batches = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0ed7fbc-33c0-44d5-8e41-b5063d6e6dde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "115668d0-fcad-4c6c-8e81-98a4ed51c77f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 1 input(s), but it received 16 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int16>, <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=int8>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:9' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:10' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:11' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:12' shape=(None,) dtype=int8>, <tf.Tensor 'IteratorGetNext:13' shape=(None, 30) dtype=int32>, <tf.Tensor 'IteratorGetNext:14' shape=(None,) dtype=int8>, <tf.Tensor 'IteratorGetNext:15' shape=(None,) dtype=string>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_df\u001b[38;5;241m.\u001b[39mdata, Y, epochs\u001b[38;5;241m=\u001b[39mnum_epochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/h4/40rl009s0kb57zjm1tj9msth0000gn/T/__autograph_generated_filen36wsvwc.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 1 input(s), but it received 16 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int16>, <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=int8>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:9' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:10' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:11' shape=(None, 100) dtype=int8>, <tf.Tensor 'IteratorGetNext:12' shape=(None,) dtype=int8>, <tf.Tensor 'IteratorGetNext:13' shape=(None, 30) dtype=int32>, <tf.Tensor 'IteratorGetNext:14' shape=(None,) dtype=int8>, <tf.Tensor 'IteratorGetNext:15' shape=(None,) dtype=string>]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_df.data, Y, epochs=num_training_steps, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af9a9fdd-42b8-45cc-bf44-1996313a8aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1  0 -1  0  0  0  0 -1  0  0 -1  0 -1  0  0 -1 -1  0  0 -1  0 -1\n",
      " -1  0  0  0 -1 -1 -1  0 -1 -1 -1  0  0 -1  0 -1  0  0  0 -1 -1 -1 -1 -1\n",
      "  0 -1  0  0  0  0  0 -1  0  0  0 -1 -1 -1  0  0  0 -1  1  0 -1 -1  0  0\n",
      "  0  1 -1  1  0 -1 -1  0  1 -1  0 -1 -1  0 -1  0  0  0 -1  0  0 -1  0  0\n",
      "  0 -1  0 -1 -1  0  0  0  0 -1  0  0  0 -1  0  1  0  0 -1 -1  0 -1  0  1\n",
      "  0  0 -1 -1 -1  0  0  0 -1  0  0 -1  0 -1  0 -1 -1 -1  0  0  0  1  1  0\n",
      "  0  0  0 -1 -1  0  0  0  0  0 -1 -1  0 -1 -1  0  0 -1 -1  0 -1  0 -1  0\n",
      "  0  0  0 -1  0  0 -1 -1 -1  0  0  0  0  0 -1  1  1  0  0  0  0  0  0 -1\n",
      " -1  0 -1  0  1 -1  1 -1 -1 -1  1  0  0 -1  0  0 -1  0  0 -1 -1 -1  0  0\n",
      "  0 -1 -1 -1 -1  0 -1  0  0  0 -1 -1  0  0 -1  0  0  1  1  0 -1  1  0  0\n",
      " -1  0  0  0  0 -1  0  0  0  0  0 -1 -1 -1  0  0  0 -1  1  0  0  0  0  0\n",
      "  0  0  0 -1 -1  0  0  0 -1 -1  0 -1  0  0  0  0 -1  0  0  0 -1 -1  0 -1\n",
      "  0  0 -1  0 -1 -1 -1  0 -1 -1  0  0 -1  1  0  0 -1  0  0 -1  0  0 -1 -1\n",
      "  1  0  0  0 -1  0  0  0  0  0  0  0  0 -1  1  0  0  0 -1 -1  0  0  0  0\n",
      "  0 -1 -1 -1  0 -1 -1 -1 -1 -1  0 -1  0  0  0 -1  0 -1  0  0 -1  0  1  0\n",
      "  0  0 -1  0  0  0 -1  0  0  0 -1 -1 -1  0 -1  0  0  0  0  1 -1  0 -1 -1\n",
      "  1  0  0  0 -1  0 -1  0 -1 -1  0  0  0  0  0 -1 -1  0  1  0 -1 -1 -1  0\n",
      "  0  0 -1  0  0 -1 -1  0 -1 -1 -1  0  0  1  0  0  0  1  0  0  0  0  1 -1\n",
      "  0  0 -1  0 -1 -1  0 -1  0  0 -1  0  0 -1  0  0 -1  1 -1  0 -1  0 -1  0\n",
      " -1  1  0 -1  0  0  0 -1 -1  1  0  0 -1 -1  0 -1 -1  0  0  0  0 -1  0  0\n",
      " -1  0  0  0 -1  0 -1  0  0  0  0 -1  0  0  0  1  0  0 -1  0 -1  0  0 -1\n",
      "  0 -1  0 -1 -1  0 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "val_batch_df = next(val_generator)\n",
    "print(val_batch_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa0b6dd7-b676-4daf-af63-a7f5dff49211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensors are unhashable (this tensor: KerasTensor(type_spec=TensorSpec(shape=(None, 100, 2), dtype=tf.float32, name=None), name='tf.one_hot_1/one_hot:0', description=\"created by layer 'tf.one_hot_1'\")). Instead, use tensor.ref() as the key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_training_steps):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Validation evaluation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     val_batch_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(val_generator)\n\u001b[0;32m----> 9\u001b[0m     val_feed_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m         is_ordered_history: val_batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_ordered_history\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m         index_in_order_history: val_batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_in_order_history\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m         order_dow_history: val_batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_dow_history\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     13\u001b[0m         order_hour_history: val_batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_hour_history\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     14\u001b[0m         days_since_prior_order_history: val_batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdays_since_prior_order_history\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     15\u001b[0m         order_size_history: val_batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_size_history\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     16\u001b[0m         reorder_size_history: val_batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreorder_size_history\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     17\u001b[0m         order_number_history: val_batch_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_number_history\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Add other inputs here\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     }\n\u001b[1;32m     20\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(val_feed_dict, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m     val_loss_history\u001b[38;5;241m.\u001b[39mappend(val_loss)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/keras_tensor.py:270\u001b[0m, in \u001b[0;36mKerasTensor.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensors are unhashable (this tensor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead, use tensor.ref() as the key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensors are unhashable (this tensor: KerasTensor(type_spec=TensorSpec(shape=(None, 100, 2), dtype=tf.float32, name=None), name='tf.one_hot_1/one_hot:0', description=\"created by layer 'tf.one_hot_1'\")). Instead, use tensor.ref() as the key."
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "best_validation_loss, best_validation_tstep = float('inf'), 0\n",
    "restarts = 0\n",
    "\n",
    "for step in range(num_training_steps):\n",
    "    # Validation evaluation\n",
    "    val_batch_df = next(val_generator)\n",
    "    val_feed_dict = {\n",
    "        is_ordered_history: val_batch_df['is_ordered_history'],\n",
    "        index_in_order_history: val_batch_df['index_in_order_history'],\n",
    "        order_dow_history: val_batch_df['order_dow_history'],\n",
    "        order_hour_history: val_batch_df['order_hour_history'],\n",
    "        days_since_prior_order_history: val_batch_df['days_since_prior_order_history'],\n",
    "        order_size_history: val_batch_df['order_size_history'],\n",
    "        reorder_size_history: val_batch_df['reorder_size_history'],\n",
    "        order_number_history: val_batch_df['order_number_history']\n",
    "        # Add other inputs here\n",
    "    }\n",
    "    val_loss = model.evaluate(val_feed_dict, verbose=0)\n",
    "    val_loss_history.append(val_loss)\n",
    "    print(val_loss)\n",
    "    \n",
    "    step += 1\n",
    "\n",
    "#     # Train step\n",
    "#     train_batch_df = next(train_generator)\n",
    "#     train_feed_dict = {\n",
    "#         is_ordered_history: train_batch_df['is_ordered_history'],\n",
    "#         index_in_order_history: train_batch_df['index_in_order_history'],\n",
    "#         order_dow_history: train_batch_df['order_dow_history'],\n",
    "#         order_hour_history: train_batch_df['order_hour_history'],\n",
    "#         days_since_prior_order_history: train_batch_df['days_since_prior_order_history'],\n",
    "#         order_size_history: train_batch_df['order_size_history'],\n",
    "#         reorder_size_history: train_batch_df['reorder_size_history'],\n",
    "#         order_number_history: train_batch_df['order_number_history']\n",
    "#         # Add other inputs here\n",
    "#     }\n",
    "#     train_loss = model.train_on_batch(train_feed_dict, verbose=0)\n",
    "#     train_loss_history.append(train_loss)\n",
    "\n",
    "#     if step % log_interval == 0:\n",
    "#         avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n",
    "#         avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n",
    "#         metric_log = (\n",
    "#             \"[[step {:>8}]]     \"\n",
    "#             \"[[train]]     loss: {:<12}     \"\n",
    "#             \"[[val]]     loss: {:<12}     \"\n",
    "#         ).format(step, round(avg_train_loss, 8), round(avg_val_loss, 8))\n",
    "#         logging.info(metric_log)\n",
    "\n",
    "#         if avg_val_loss < best_validation_loss:\n",
    "#             best_validation_loss = avg_val_loss\n",
    "#             best_validation_tstep = step\n",
    "#             if step > min_steps_to_checkpoint:\n",
    "#                 model.save_weights('model_checkpoint.h5')\n",
    "#                 if enable_parameter_averaging:\n",
    "#                     # Save averaged weights if needed\n",
    "#                     pass\n",
    "\n",
    "#     if step - best_validation_tstep > early_stopping_steps:\n",
    "#         if num_restarts is None or restarts >= num_restarts:\n",
    "#             logging.info('best validation loss of {} at training step {}'.format(\n",
    "#                 best_validation_loss, best_validation_tstep))\n",
    "#             logging.info('early stopping - ending training.')\n",
    "#             break\n",
    "\n",
    "#         if restarts < num_restarts:\n",
    "#             model.load_weights('model_checkpoint.h5')\n",
    "#             logging.info('halving learning rate')\n",
    "#             new_lr = model.optimizer.learning_rate / 2.0\n",
    "#             tf.keras.backend.set_value(model.optimizer.learning_rate, new_lr)\n",
    "#             early_stopping_steps /= 2\n",
    "#             step = best_validation_tstep\n",
    "#             restarts += 1\n",
    "\n",
    "# if step <= min_steps_to_checkpoint:\n",
    "#     best_validation_tstep = step\n",
    "#     model.save_weights('final_model.h5')\n",
    "#     if enable_parameter_averaging:\n",
    "#         # Save averaged weights if needed\n",
    "#         pass\n",
    "\n",
    "# logging.info('num_training_steps reached - ending training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
